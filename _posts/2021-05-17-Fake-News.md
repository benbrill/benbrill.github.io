---
layout: post
title: Classifying Fake News Using Neural Networks
image: benbrill.github.io\images\ucla-math.png
---

COVID-19 is not the only pandemic we are facing. Though it is far less deadly, our country and our world has been experiencing an epidemic of fake news for the past few years. The rising of use of social media in political campaigns has slowly but surely introduced more and more false information into our news ecosystem, as to convince the masses to form one opinion over another. It has already substantially affected a few elections, and could very well affect more in the near future.

Rooting out this problem is not that difficult; just check the facts of the story to determine if it is real or not. However, in practice, that can be impractical to readers who these stories are targeted at, who often simply look at the headline to get the gist of the story.

What if there was some way to classify these stories as real or fake for readers before they view it, and to do this without verifying each and every fact in the story but simply to look at patterns in the headlines and text? Enter **neural networks**.

## Setting up the data
First, let's import the whole boatload of libraries that are necessary to do this. 


```python
# basic libraries
import pandas as pd
import tensorflow as tf
from nltk.corpus import stopwords
from nltk import download
import numpy as np
import string
import re

# libraries to build model
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

# specialized layers to format data appropriate for the model
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup
```

We will also must download a list of stopwords (a, the, but, and, etc.) that we will use later


```python
download('stopwords')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!
    




    True



Let's load in our data from the url below


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
data = pd.read_csv(train_url)
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>



We see we have a data frame with each row representing one story. We have a `title` column which is the headline for each story, the text of the story in the `text` column, and whether the story was fake or not in the `fake` column, with a `1` denoting that the story is fake.

### Preprocessing Text
In order to get the best result out of our model, we only want to give it words to analyze that are import. "Stopwords," like "but," "and," or "I," really don't have any particular meaning nor give any weight to determining whether a story is real or not. So we will get rid of each of these words in the `title` and `text` columns.

In addition, we are going to convert our dataframe columns into a `tensorflow` Dataset, which is essentially an easier way for our neural networks to read our data.


```python
def make_dataset(df):
  stop = stopwords.words('english')
  df = df.copy()
  df['title'] = df['title'].apply(lambda x: ' '.join(item for item in x.split() if item not in stop))
  df['text'] = df['text'].apply(lambda x: ' '.join(item for item in x.split() if item not in stop))

  data = tf.data.Dataset.from_tensor_slices((
      # predictor data
      {
          "title": df[["title"]],
          "text": df[["text"]]
      },
      {
          "fake": df[["fake"]]
      }
  ))

  return data

```


```python
tfData = make_dataset(data)
```

### Train-test split
Now that we have our dataset set up, we are going to split it into a data set for training, a data set for validation, as well as a data set for testing and evaluation. Since our data is formated in a special `tensorflow` Dataset, we will use the `take()` method.


```python
tfData = tfData.shuffle(buffer_size = len(tfData))

train_size = int(0.7*len(tfData))
val_size   = int(0.1*len(tfData))

train = tfData.take(train_size).batch(20)
val   = tfData.skip(train_size).take(val_size).batch(20)
test  = tfData.skip(train_size + val_size).batch(20)

len(train), len(val), len(test)
```




    (786, 113, 225)



### Creating Vectorized text
Finally, we are going to standardize each of our columns with text by converting all words to lower case and removing any punctuation.

Once that is done, we will vectorize our text. What this means is that we are going to seperate an entire string of words into seperate vector components, with a numerical representation of each word. Since computers only understand numbers, this is a very important step


```python
size_vocabulary = 2000
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 


```

We can see what this vectorization looks like below once we adapt each layer to a specific purpose

# Title Model

We are not going to create a model that can predict whether a story is real or fake based off it's headline. First, we must tell our vecorization layer we want it to vectorize our `title` column


```python
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

We must now specify the input for our model. In this case it will be the `title` of a story. We can use the special `keras.Input` fclass to specify it's name, dimensions, and its data type.


```python
titleInput = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)
```

Now that we have an input, we can create a neural network. Neural Networks are comprised of layers of various types, each with a different function. In this model, we will be using the `vectorize_layer`, which vectorizes our text, and `Embedding` layer, which create a vector based off of the vectorization layer who's magnitude supposedly corresponds to the word's significane in the context of fake news. The `10` in the parameters in that layer signifies we want to have vectors in 10 dimensional space, which is sort of an arbitray number but can yield good results.

Then, we will use `Dropout` layers to drop a certain amount of data if overfitting is occuring, and `GlobalAveragePooling1D` layers to convert our 2D vectors into something more digestable.


```python
titleModel = vectorize_layer(titleInput)
titleModel = layers.Embedding(size_vocabulary, 10, name = "embedding")(titleModel)
titleModel = layers.Dropout(0.2)(titleModel)
titleModel = layers.GlobalAveragePooling1D()(titleModel)
titleModel = layers.Dropout(0.2)(titleModel)
titleModel = layers.Dense(32, activation='relu')(titleModel)
```

Finally, we will create an output layer with a size of 2, which is the number of outcomes we have, real or fake news. 


```python
output = layers.Dense(2, name = "fake")(titleModel)
```

Now, we can create the model by specifying its `keras.Inputs` as well as its output


```python
model = keras.Model(
    inputs = [titleInput],
    outputs = output
)
```


```python
model.summary()
```

    Model: "model_7"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization_1 (TextVe (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout_10 (Dropout)         (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d_5 ( (None, 10)                0         
    _________________________________________________________________
    dropout_11 (Dropout)         (None, 10)                0         
    _________________________________________________________________
    dense_6 (Dense)              (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________
    

Now that we have a pipeline set up, we can compile our model and fit it according to our training data.


```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```

    Epoch 1/10
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    
    

    786/786 [==============================] - 4s 5ms/step - loss: 0.6807 - accuracy: 0.5516 - val_loss: 0.3782 - val_accuracy: 0.9104
    Epoch 2/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.2685 - accuracy: 0.9328 - val_loss: 0.1273 - val_accuracy: 0.9559
    Epoch 3/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.1170 - accuracy: 0.9612 - val_loss: 0.0986 - val_accuracy: 0.9635
    Epoch 4/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0889 - accuracy: 0.9689 - val_loss: 0.0577 - val_accuracy: 0.9768
    Epoch 5/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0669 - accuracy: 0.9755 - val_loss: 0.0542 - val_accuracy: 0.9835
    Epoch 6/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0627 - accuracy: 0.9787 - val_loss: 0.0537 - val_accuracy: 0.9782
    Epoch 7/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.0525 - val_accuracy: 0.9844
    Epoch 8/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0591 - accuracy: 0.9799 - val_loss: 0.0456 - val_accuracy: 0.9835
    Epoch 9/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0510 - accuracy: 0.9821 - val_loss: 0.0371 - val_accuracy: 0.9884
    Epoch 10/10
    786/786 [==============================] - 4s 5ms/step - loss: 0.0454 - accuracy: 0.9835 - val_loss: 0.0422 - val_accuracy: 0.9862
    

Our model does pretty well on both training and validation data! Let's see how a similar model will perform on the `text` of a story

# Text Model


```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```


```python
textModel = vectorize_layer(textInput)
textModel = layers.Embedding(size_vocabulary, 10, name = "embedding")(textModel)
textModel = layers.Dropout(0.2)(textModel)
textModel = layers.GlobalAveragePooling1D()(textModel)
textModel = layers.Dropout(0.2)(textModel)
textModel = layers.Dense(32, activation='relu')(textModel)
```


```python
output = layers.Dense(2, name = "fake")(textModel)
model = keras.Model(
    inputs = [textInput],
    outputs = output
)
```


```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
textHistory = model.fit(train, 
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```

    Epoch 1/10
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:
    
    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
    
    

    786/786 [==============================] - 6s 7ms/step - loss: 0.5709 - accuracy: 0.6932 - val_loss: 0.1613 - val_accuracy: 0.9523
    Epoch 2/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.1611 - accuracy: 0.9542 - val_loss: 0.1045 - val_accuracy: 0.9746
    Epoch 3/10
    786/786 [==============================] - 5s 6ms/step - loss: 0.1052 - accuracy: 0.9704 - val_loss: 0.1087 - val_accuracy: 0.9697
    Epoch 4/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.0957 - accuracy: 0.9724 - val_loss: 0.0916 - val_accuracy: 0.9742
    Epoch 5/10
    786/786 [==============================] - 5s 6ms/step - loss: 0.0812 - accuracy: 0.9751 - val_loss: 0.0725 - val_accuracy: 0.9746
    Epoch 6/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.0706 - accuracy: 0.9785 - val_loss: 0.0583 - val_accuracy: 0.9831
    Epoch 7/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.0603 - accuracy: 0.9831 - val_loss: 0.0630 - val_accuracy: 0.9799
    Epoch 8/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.0634 - accuracy: 0.9804 - val_loss: 0.0518 - val_accuracy: 0.9844
    Epoch 9/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.0518 - accuracy: 0.9843 - val_loss: 0.0454 - val_accuracy: 0.9853
    Epoch 10/10
    786/786 [==============================] - 5s 7ms/step - loss: 0.0470 - accuracy: 0.9856 - val_loss: 0.0691 - val_accuracy: 0.9639
    

# Title and Text Model


```python
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```


```python
titleInput = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

textInput = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```


```python
main = layers.concatenate([titleModel, textModel], axis = 1)
```


```python
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)
```


```python
model = keras.Model(
    inputs = [titleInput, textInput],
    outputs = output
)
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-32-006de513692e> in <module>()
          1 model = keras.Model(
          2     inputs = [titleInput, textInput],
    ----> 3     outputs = output
          4 )
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
        515     self._self_setattr_tracking = False  # pylint: disable=protected-access
        516     try:
    --> 517       result = method(self, *args, **kwargs)
        518     finally:
        519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable, **kwargs)
        118     generic_utils.validate_kwargs(kwargs, {})
        119     super(Functional, self).__init__(name=name, trainable=trainable)
    --> 120     self._init_graph_network(inputs, outputs)
        121 
        122   @trackable.no_automatic_dependency_tracking
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
        515     self._self_setattr_tracking = False  # pylint: disable=protected-access
        516     try:
    --> 517       result = method(self, *args, **kwargs)
        518     finally:
        519       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)
        202     # Keep track of the network's nodes and layers.
        203     nodes, nodes_by_depth, layers, _ = _map_graph_network(
    --> 204         self.inputs, self.outputs)
        205     self._network_nodes = nodes
        206     self._nodes_by_depth = nodes_by_depth
    

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)
        988                              'The following previous layers '
        989                              'were accessed without issue: ' +
    --> 990                              str(layers_with_complete_input))
        991         for x in nest.flatten(node.outputs):
        992           computable_tensors.add(id(x))
    

    ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='text'), name='text', description="created by layer 'text'") at layer "text_vectorization". The following previous layers were accessed without issue: []



```python
model.summary()
```


```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 10, 
                    verbose = True)
```

# Validation


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
testingData = pd.read_csv(test_url)
testingData
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>420</td>
      <td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
      <td>Donald Trump practically does something to cri...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14902</td>
      <td>Exclusive: Kremlin tells companies to deliver ...</td>
      <td>The Kremlin wants good news.  The Russian lead...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>322</td>
      <td>Golden State Warriors Coach Just WRECKED Trum...</td>
      <td>On Saturday, the man we re forced to call  Pre...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16108</td>
      <td>Putin opens monument to Stalin's victims, diss...</td>
      <td>President Vladimir Putin inaugurated a monumen...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10304</td>
      <td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
      <td>Apparently breaking the law and scamming the g...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>20058</td>
      <td>U.S. will stand be steadfast ally to Britain a...</td>
      <td>The United States will stand by Britain as it ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>21104</td>
      <td>Trump rebukes South Korea after North Korean b...</td>
      <td>U.S. President Donald Trump admonished South K...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>2842</td>
      <td>New rule requires U.S. banks to allow consumer...</td>
      <td>U.S. banks and credit card companies could be ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>22298</td>
      <td>US Middle Class Still Suffering from Rockefell...</td>
      <td>Dick Eastman The Truth HoundWhen Henry Kissin...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>333</td>
      <td>Scaramucci TV Appearance Goes Off The Rails A...</td>
      <td>The most infamous characters from Donald Trump...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>




```python
testingData = make_dataset(testingData)
```


```python
model.evaluate(testingData)
```

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      [n for n in tensors.keys() if n not in ref_input_names])
    

    22449/22449 [==============================] - 52s 2ms/step - loss: 2.4460 - accuracy: 0.5038
    




    [2.458550214767456, 0.5039422512054443]




```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
import numpy as np

fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```




```python
weights
```




    array([[ 1.6684402e-02, -5.2643596e-04],
           [-8.7498590e-02, -4.0375008e-03],
           [ 1.9110104e+00,  7.6016746e-02],
           ...,
           [-2.6335064e-01, -6.2618703e-02],
           [-2.7115735e-01, -7.5890809e-02],
           [-9.9839546e-02, -4.5861937e-02]], dtype=float32)




```python

```
